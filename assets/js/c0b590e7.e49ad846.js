"use strict";(self.webpackChunkprem_docs=self.webpackChunkprem_docs||[]).push([[7282],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),d=i,m=u["".concat(l,".").concat(d)]||u[d]||h[d]||r;return n?a.createElement(m,o(o({ref:t},c),{},{components:n})):a.createElement(m,o({ref:t},c))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:i,o[1]=s;for(var p=2;p<r;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},36289:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var a=n(87462),i=(n(67294),n(3905));const r={slug:"teach-chatbot-with-audio",title:"Teach a Q&A Chatbot with Audio Recordings",authors:["het","casperdcl"],tags:["llm","self-hosted","prem","open-source","fintech","langchain","vicuna-7b","weaviate","vector-store","streamlit"],description:"Build a chatbot to answer questions about audio recordings with Prem using LangChain, Whisper audio transcription, All MiniLM embeddings, Weaviate vector store and Vicuna 7B LLM, self-hosted on your laptop",image:"./banner.png"},o=void 0,s={permalink:"/blog/teach-chatbot-with-audio",editUrl:"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-08-03-teach-chatbot-with-audio/index.md",source:"@site/blog/2023-08-03-teach-chatbot-with-audio/index.md",title:"Teach a Q&A Chatbot with Audio Recordings",description:"Build a chatbot to answer questions about audio recordings with Prem using LangChain, Whisper audio transcription, All MiniLM embeddings, Weaviate vector store and Vicuna 7B LLM, self-hosted on your laptop",date:"2023-08-03T00:00:00.000Z",formattedDate:"August 3, 2023",tags:[{label:"llm",permalink:"/blog/tags/llm"},{label:"self-hosted",permalink:"/blog/tags/self-hosted"},{label:"prem",permalink:"/blog/tags/prem"},{label:"open-source",permalink:"/blog/tags/open-source"},{label:"fintech",permalink:"/blog/tags/fintech"},{label:"langchain",permalink:"/blog/tags/langchain"},{label:"vicuna-7b",permalink:"/blog/tags/vicuna-7-b"},{label:"weaviate",permalink:"/blog/tags/weaviate"},{label:"vector-store",permalink:"/blog/tags/vector-store"},{label:"streamlit",permalink:"/blog/tags/streamlit"}],readingTime:5.225,hasTruncateMarker:!0,authors:[{name:"Het Trivedi",title:"Developer Advocate",url:"https://github.com/htrivedi99",imageURL:"https://github.com/htrivedi99.png",key:"het"},{name:"Casper da Costa-Luis",title:"Core contributor @ PremAI",url:"https://github.com/casperdcl",imageURL:"https://github.com/casperdcl.png",key:"casperdcl"}],frontMatter:{slug:"teach-chatbot-with-audio",title:"Teach a Q&A Chatbot with Audio Recordings",authors:["het","casperdcl"],tags:["llm","self-hosted","prem","open-source","fintech","langchain","vicuna-7b","weaviate","vector-store","streamlit"],description:"Build a chatbot to answer questions about audio recordings with Prem using LangChain, Whisper audio transcription, All MiniLM embeddings, Weaviate vector store and Vicuna 7B LLM, self-hosted on your laptop",image:"./banner.png"},prevItem:{title:"MLOps: More Oops than Ops",permalink:"/blog/mlops-more-oops-than-ops"},nextItem:{title:"Talk to your Data with ChainLit and Langchain",permalink:"/blog/chainlit-langchain-prem"}},l={image:n(71120).Z,authorsImageUrls:[void 0,void 0]},p=[{value:"Financial Use Case",id:"financial-use-case",level:2},{value:"Processing Audio with AI",id:"processing-audio-with-ai",level:2},{value:"Step 1: Setup Requirements",id:"step-1-setup-requirements",level:2},{value:"Step 2: Transcribe Audio to\xa0Text",id:"step-2-transcribe-audio-totext",level:2},{value:"Step 3: Split Text into Chunks",id:"step-3-split-text-into-chunks",level:2},{value:"Step 4: Save Chunks into Vector\xa0Store",id:"step-4-save-chunks-into-vectorstore",level:2},{value:"Step 5: Query the Vector\xa0Store",id:"step-5-query-the-vectorstore",level:2},{value:"Step 6: Combine Everything in a ChatBot GUI",id:"step-6-combine-everything-in-a-chatbot-gui",level:2},{value:"Conclusion",id:"conclusion",level:2}],c={toc:p},u="wrapper";function h(e){let{components:t,...r}=e;return(0,i.kt)(u,(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Prem Banner",src:n(71120).Z,width:"2238",height:"675"})),(0,i.kt)("head",null,(0,i.kt)("meta",{name:"twitter:image",content:"./banner.png"})),(0,i.kt)("h2",{id:"financial-use-case"},"Financial Use Case"),(0,i.kt)("p",null,"Publicly traded companies are required to report to their investors. In the US, companies typically have a live call with their investors before publishing quarterly (",(0,i.kt)("a",{parentName:"p",href:"https://www.investor.gov/introduction-investing/investing-basics/glossary/form-10-q"},"10-Q"),") and annual (",(0,i.kt)("a",{parentName:"p",href:"https://www.investor.gov/introduction-investing/investing-basics/glossary/form-10-k"},"10-K"),") earnings reports. People on the call receive important information quicker than those who wait for written reports."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Can we capture all the information from a live earnings call without having to sit through the entire thing or wait for official written reports?")),(0,i.kt)("p",null,"Yes!"),(0,i.kt)("h2",{id:"processing-audio-with-ai"},"Processing Audio with AI"),(0,i.kt)("p",null,"The open-source ",(0,i.kt)("a",{parentName:"p",href:"https://doi.org/10.48550/arXiv.2212.04356"},"Whisper Tiny")," model can convert audio into text. This model is small enough to run on your CPU and can process hours of audio quickly (in seconds) and accurately."),(0,i.kt)("p",null,"We can then use an open-source large language model (LLM) such as ",(0,i.kt)("a",{parentName:"p",href:"https://vicuna.lmsys.org"},"Vicuna"),' to query or "chat" with the text in a more human-friendly way than a basic ',(0,i.kt)("inlineCode",{parentName:"p"},"Ctrl + F"),' ever could. We could ask questions like "where has the revenue grown most this year?" or "what was the net revenue for the quarter?"'),(0,i.kt)("p",null,"The steps required to build this app are:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Serve the Whisper Tiny audio-to-text model locally"),(0,i.kt)("li",{parentName:"ol"},"Send our audio file to the model and receive the resulting text"),(0,i.kt)("li",{parentName:"ol"},"Split the text into manageable chunks using ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/langchain-ai/langchain"},"LangChain")),(0,i.kt)("li",{parentName:"ol"},"Store the chunks inside a vector database provided by ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/weaviate/weaviate"},"Weaviate")),(0,i.kt)("li",{parentName:"ol"},"Query the data stored in the vector database"),(0,i.kt)("li",{parentName:"ol"},"Chat with the data using the Vicuna LLM")),(0,i.kt)("p",null,"There are quite a few technologies here and it can be difficult to run them all manually. We'll use Prem AI to easily handle most of this without having to wrestle with any setup -- including the Whisper Tiny model, Weaviate vector database, embeddings tool, and the Vicuna LLM."),(0,i.kt)("h2",{id:"step-1-setup-requirements"},"Step 1: Setup Requirements"),(0,i.kt)("p",null,"Simply ",(0,i.kt)("a",{parentName:"p",href:"https://dev.premai.io/docs/category/installation"},"install & run the Prem App"),". Using the app's UI, start these services:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://registry.premai.io/detail.html?service=whisper-tiny"},"Whisper Tiny")," (under ",(0,i.kt)("em",{parentName:"li"},"Audio-to-Text"),"),"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://registry.premai.io/detail.html?service=all-minilm-l6-v2"},"All MiniLM L6 v2")," (under ",(0,i.kt)("em",{parentName:"li"},"Embeddings"),"),"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://registry.premai.io/detail.html?service=weaviate"},"Weaviate")," (under ",(0,i.kt)("em",{parentName:"li"},"Vector-Store"),"), and"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://registry.premai.io/detail.html?service=vicuna-7b-q4"},"Vicun-7B q4")," (under ",(0,i.kt)("em",{parentName:"li"},"Chat"),").")),(0,i.kt)("p",null,"Note that there are many other services you could select from instead (e.g. a larger ",(0,i.kt)("em",{parentName:"p"},"Chat")," model if your GPU memory is large enough)."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(88090).Z,width:"2576",height:"1348"})," "),(0,i.kt)("p",null,"Each service runs in a Docker container, and can be started/stopped via the app."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(9698).Z,width:"976",height:"1306"})," "),(0,i.kt)("p",null,"Click on a running service to see usage docs and Docker container info. Notice the ",(0,i.kt)("inlineCode",{parentName:"p"},"Default External Port"),", which we will use below to interact with each of these services via their API endpoints."),(0,i.kt)("p",null,"Next, install some Python dependencies:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"python -m pip install langchain openai streamlit\n")),(0,i.kt)("h2",{id:"step-2-transcribe-audio-totext"},"Step 2: Transcribe Audio to\xa0Text"),(0,i.kt)("p",null,"We'll create a ",(0,i.kt)("inlineCode",{parentName:"p"},"convert_audio_to_text")," function which sends a given audio file to our Prem AI managed audio-to-text model, and returns the transcribed text."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport openai\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# URL API endpoints obtained from the Prem App UI\nopenai.api_key = os.environ["OPENAI_API_KEY"] = "random-string"\nwhisper_url = "http://127.0.0.1:10111/v1" # audio-to-text\nembeddings = OpenAIEmbeddings(openai_api_base="http://127.0.0.1:8444/v1")\nweaviate_url = "http://127.0.0.1:8080" # vector store\nvicuna_api = "http://127.0.0.1:8111/v1" # LLM\n\ndef convert_audio_to_text(audio_file_path) -> str:\n    openai.api_base = whisper_url\n    with open(audio_file_path, "rb") as audio_file:\n        transcript = openai.Audio.transcribe("whisper-1", audio_file)\n        return transcript.get("text")\n')),(0,i.kt)("h2",{id:"step-3-split-text-into-chunks"},"Step 3: Split Text into Chunks"),(0,i.kt)("p",null,"We define a ",(0,i.kt)("inlineCode",{parentName:"p"},"create_chunks")," function to split the text into smaller pieces which are much easier to process."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef create_chunks(text: str):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500, chunk_overlap=20, length_function=len)\n    return text_splitter.create_documents([text])\n")),(0,i.kt)("h2",{id:"step-4-save-chunks-into-vectorstore"},"Step 4: Save Chunks into Vector\xa0Store"),(0,i.kt)("p",null,"An ",(0,i.kt)("inlineCode",{parentName:"p"},"add_to_vectorstore")," function will save the chunks in our Prem-managed vector store, making them easy to query later."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.docstore.document import Document\nfrom langchain.vectorstores import VectorStore, Weaviate\n\ndef add_to_vectorstore(texts) -> VectorStore:\n    documents = [Document(page_content=t.page_content) for t in texts]\n    return Weaviate.from_documents(\n        documents, embeddings, weaviate_url=weaviate_url, by_text=False)\n")),(0,i.kt)("p",null,"Note that the All MiniLM L6 v2 ",(0,i.kt)("inlineCode",{parentName:"p"},"embeddings")," endpoint is used to convert our text into vectors."),(0,i.kt)("h2",{id:"step-5-query-the-vectorstore"},"Step 5: Query the Vector\xa0Store"),(0,i.kt)("p",null,"We can query against them using the ",(0,i.kt)("inlineCode",{parentName:"p"},"similarity_search_by_vector")," function."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def query_vectorstore(query: str, vectorstore: VectorStore) -> str:\n    query_vector = embeddings.embed_query(query)\n    docs = vectorstore.similarity_search_by_vector(query_vector, k=1)\n    return "\\n\\n".join(doc.page_content for doc in docs)\n')),(0,i.kt)("p",null,"Here's how this works:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The input query (our question) is converted into a vector using ",(0,i.kt)("inlineCode",{parentName:"li"},"embeddings.embed_query")),(0,i.kt)("li",{parentName:"ul"},"The vector database is searched for similar vectors"),(0,i.kt)("li",{parentName:"ul"},"The original text (corresponding to those vectors) is returned")),(0,i.kt)("h2",{id:"step-6-combine-everything-in-a-chatbot-gui"},"Step 6: Combine Everything in a ChatBot GUI"),(0,i.kt)("p",null,"We'll select an ",(0,i.kt)("inlineCode",{parentName:"p"},"mp3")," audio file to use via a Streamlit UI. In this case, we'll use ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/htrivedi99/prem-blogs/blob/main/quarterly-earnings-chatbot/nvidia_earnings_call.mp3"},"this audio snippet")," extracted from ",(0,i.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=7qU_wzzYNJU"},"NVIDIA's recent Q4 2023 earnings call"),"."),(0,i.kt)("p",null,"Then we'll use the functions defined above to support an LLM-driven chatbot!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import tempfile\nimport streamlit as st\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nchat = ChatOpenAI(openai_api_base=vicuna_api, max_tokens=256)\nst.title("Chat with audio files")\nwith st.sidebar:\n    upload_file = st.file_uploader("Choose an audio file")\n    if upload_file is not None:\n        with tempfile.NamedTemporaryFile() as tmp:\n            tmp.write(upload_file.read())\n            st.write("Converting audio to text...")\n            text = convert_audio_to_text(tmp.name)\n            st.write("... done!")\n            chunks = create_chunks(text)\n            vector_db = add_to_vectorstore(chunks)\n\nuser_input = st.text_input("Enter your question here...")\nif user_input:\n    context = query_vectorstore(user_input, vector_db)\n    messages = [HumanMessage(content=(\n        "Use the context below to answer the question.\\n"\n        f"Context: {context}. \\nQuestion: {user_input}"))]\n    st.write("Generating...")\n    res = chat(messages)\n    st.write(res.content)\n')),(0,i.kt)("p",null,"To answer our questions correctly, we need to provide the LLM with some context.\nThis context is given by the text we obtained from our vector database from a similarity search (finding transcribed audio extracts similar to our question)."),(0,i.kt)("p",null,"To run everything, place all the Python code above into a file ",(0,i.kt)("inlineCode",{parentName:"p"},"audiobot.py")," and run it using ",(0,i.kt)("inlineCode",{parentName:"p"},"streamlit"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"streamlit run audiobot.py\n")),(0,i.kt)("p",null,"A browser window should open displaying your shiny new audio transcription chatbot app!"),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/htrivedi99/prem-blogs/tree/main/quarterly-earnings-chatbot"},"full source code for this tutorial is available here"),"."),(0,i.kt)("h2",{id:"conclusion"},"Conclusion"),(0,i.kt)("p",null,"We took a practical use case -- extracting information from an earnings call -- and applied state-of-the-art AI techniques to convert the audio stream into text, and load the text into a vector database. We asked questions about the text in natural (human) language, and used a vector similarity search and an LLM to answer these questions."),(0,i.kt)("p",null,"The applications for this project go well beyond just earnings calls, as any kind of audio can be processed. For example, you can summarise a video recording from a meeting, lecture, or even YouTube."),(0,i.kt)("p",null,"Using Prem AI you can easily experiment with different open-source AI building blocks to help you build applications for your particular use case."))}h.isMDXComponent=!0},71120:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/banner-2dab08c15689cf175a9b63e7b940916e.png"},88090:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/prem_dashboard-67603b3e65d27f8fccb07c3a69baf9e2.png"},9698:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/prem_service_details-092b5a10f78f03b6eca6f85e11d2de6a.png"}}]);